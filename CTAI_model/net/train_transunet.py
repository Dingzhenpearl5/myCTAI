"""
TransUNetè®­ç»ƒè„šæœ¬
åœ¨åŸæœ‰UNet+AttentionGateåŸºç¡€ä¸Šå‡çº§ä¸ºTransUNetæ¶æ„
"""

import sys
sys.path.append("..")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from torch.cuda.amp import autocast, GradScaler  # æ··åˆç²¾åº¦è®­ç»ƒ

from data_set import make
from net import transunet
from utils import dice_loss
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

# ==================== é…ç½®å‚æ•° ====================
class Config:
    # æ•°æ®é…ç½®
    train_dataset_path = 'C:/Users/Masoa/OneDrive/work/CTAI/src/train'
    rate = 0.50  # äºŒå€¼åŒ–é˜ˆå€¼
    
    # æ¨¡å‹é…ç½®
    model_type = 'transunet_lite'  # æ”¹ç”¨Liteç‰ˆæœ¬ï¼Œæ›´å®¹æ˜“è®­ç»ƒ
    model_save_name = 'transunet_lite_622split'  # ä¿å­˜çš„æ¨¡å‹åç§°
    img_size = 512
    patch_size = 16
    embed_dim = 768  # 384 for lite version
    depth = 12       # 6 for lite version
    num_heads = 12   # 6 for lite version
    
    # è®­ç»ƒé…ç½®
    epochs = 50
    batch_size = 2  # RTX 3050 4GBæ˜¾å­˜,å»ºè®®2
    learning_rate = 1e-4  # é™ä½å­¦ä¹ ç‡,æ›´ç¨³å®š
    weight_decay = 1e-4
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_type = 'cosine'  # 'cosine' æˆ– 'plateau'
    
    # æŸå¤±å‡½æ•°æƒé‡ï¼ˆä¸UNetä¿æŒä¸€è‡´ï¼‰
    dice_weight = 1.0
    bce_weight = 0.3
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    use_amp = True  # æ˜¾å­˜ä¸è¶³æ—¶å¿…é¡»å¼€å¯
    
    # Early Stopping
    patience = 10
    min_delta = 0.001
    
    # ä¿å­˜è·¯å¾„
    save_dir = '../checkpoints'
    log_dir = '../logs'


config = Config()

# ==================== è®¾å¤‡é…ç½® ====================
torch.set_num_threads(1)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()

print(f"{'='*60}")
print("TransUNetè®­ç»ƒå¯åŠ¨")
print(f"{'='*60}")
print(f"è®¾å¤‡: {device}")
print(f"æ¨¡å‹ç±»å‹: {config.model_type}")
print(f"æ‰¹å¤§å°: {config.batch_size}")
print(f"å­¦ä¹ ç‡: {config.learning_rate}")
print(f"æ€»Epochs: {config.epochs}")
print(f"æ··åˆç²¾åº¦: {'å¯ç”¨' if config.use_amp else 'ç¦ç”¨'}")
print(f"{'='*60}\n")

# ==================== æ•°æ®åŠ è½½ ====================
print("åŠ è½½æ•°æ®é›†...")
train_dataset, val_dataset, test_dataset = make.get_d1(config.train_dataset_path)
train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

print(f"æ¯epochæ­¥æ•°: {len(train_loader)}\n")

# ==================== æ¨¡å‹åˆå§‹åŒ– ====================
print("åˆå§‹åŒ–æ¨¡å‹...")
if config.model_type == 'transunet_lite':
    model = transunet.TransUNetLite(
        img_size=config.img_size,
        patch_size=config.patch_size,
        in_channels=1,
        out_channels=1
    ).to(device)
else:
    model = transunet.TransUNet(
        img_size=config.img_size,
        patch_size=config.patch_size,
        in_channels=1,
        out_channels=1,
        embed_dim=config.embed_dim,
        depth=config.depth,
        num_heads=config.num_heads
    ).to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ€»å‚æ•°é‡: {total_params / 1e6:.2f}M")
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.2f}M\n")

# ==================== æŸå¤±å‡½æ•° ====================
# æ³¨æ„: æ¨¡å‹è¾“å‡ºå·²ç»è¿‡Sigmoid,ä½¿ç”¨BCE Loss
# ä½†æ··åˆç²¾åº¦è®­ç»ƒéœ€è¦ä½¿ç”¨ç¨³å®šçš„ç‰ˆæœ¬
criterion_bce = nn.BCELoss().to(device)

def dice_loss_fn(pred, target, smooth=1.0):
    """Dice Loss - ä¿®å¤ç‰ˆæœ¬"""
    # å±•å¹³ä¸º1Då‘é‡è¿›è¡Œè®¡ç®—
    pred_flat = pred.contiguous().view(-1)
    target_flat = target.contiguous().view(-1)
    
    intersection = (pred_flat * target_flat).sum()
    dice_score = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)
    
    return 1 - dice_score

def combined_loss(pred, target):
    """ç»„åˆæŸå¤±: BCE + Dice (å…¼å®¹æ··åˆç²¾åº¦è®­ç»ƒ)"""
    # ä¸ºäº†å…¼å®¹æ··åˆç²¾åº¦,å…ˆè½¬ä¸ºfloat32å†è®¡ç®—BCE
    with torch.cuda.amp.autocast(enabled=False):
        pred_fp32 = pred.float()
        target_fp32 = target.float()
        loss_bce = criterion_bce(pred_fp32, target_fp32)
    
    # Dice Losså¯ä»¥ç›´æ¥ç”¨æ··åˆç²¾åº¦
    loss_dice = dice_loss_fn(pred, target)
    total_loss = config.bce_weight * loss_bce + config.dice_weight * loss_dice
    return total_loss, loss_bce.item(), loss_dice.item()

# ==================== ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ====================
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=config.learning_rate,
    weight_decay=config.weight_decay
)

if config.scheduler_type == 'cosine':
    scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=1e-6)
else:
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)

# æ··åˆç²¾åº¦scaler
scaler = GradScaler() if config.use_amp else None

# ==================== Early Stopping ====================
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_dice = 0
        self.early_stop = False
        
    def __call__(self, dice):
        if dice < self.best_dice + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_dice = dice
            self.counter = 0
        return self.early_stop

early_stopping = EarlyStopping(patience=config.patience, min_delta=config.min_delta)

# ==================== è®­ç»ƒè®°å½• ====================
history = {
    'epoch': [],
    'train_loss': [],
    'train_dice': [],
    'val_dice': [],
    'learning_rate': [],
    'bce_loss': [],
    'dice_loss': []
}

best_dice = 0.0

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_one_epoch(epoch):
    model.train()
    epoch_loss = 0
    epoch_dice = 0
    epoch_bce = 0
    epoch_dice_loss = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{config.epochs}")
    
    for batch_idx, (x, mask) in enumerate(pbar):
        # DataLoaderå·²ç»è‡ªåŠ¨ç»„å¥½batchäº†
        # xæ˜¯list: [images_batch, patient_ids, filenames]
        # maskæ˜¯list: [filenames, masks_batch]
        # images_batch shape: (B, 1, 512, 512)
        # masks_batch shape: (B, 512, 512)
        
        images = x[0].to(device)  # (B, 1, 512, 512)
        targets = mask[1].unsqueeze(1).to(device)  # (B, 512, 512) -> (B, 1, 512, 512)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        if config.use_amp:
            with autocast():
                outputs = model(images)
                loss, loss_bce, loss_dice = combined_loss(outputs, targets)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(images)
            loss, loss_bce, loss_dice = combined_loss(outputs, targets)
            loss.backward()
            optimizer.step()
        
        # è®¡ç®—Dice
        with torch.no_grad():
            pred_np = outputs.cpu().detach().squeeze(1).numpy()
            target_np = targets.cpu().detach().squeeze(1).numpy()
            
            # ç¡®ä¿æ˜¯3Dæ•°ç»„(batch, H, W)
            if pred_np.ndim == 2:
                pred_np = pred_np[np.newaxis, ...]
            if target_np.ndim == 2:
                target_np = target_np[np.newaxis, ...]
            
            # è®¡ç®—batchå¹³å‡Dice
            batch_dice = 0
            for i in range(pred_np.shape[0]):
                pred_binary = (pred_np[i] >= config.rate).astype(np.float32)
                target_binary = target_np[i]
                batch_dice += dice_loss.dice(pred_binary, target_binary)
            batch_dice /= pred_np.shape[0]
        
        epoch_loss += loss.item()
        epoch_dice += batch_dice
        epoch_bce += loss_bce
        epoch_dice_loss += loss_dice
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Dice': f'{batch_dice:.4f}',
            'LR': f'{optimizer.param_groups[0]["lr"]:.6f}'
        })
    
    # è®¡ç®—å¹³å‡å€¼
    avg_loss = epoch_loss / len(train_loader)
    avg_dice = epoch_dice / len(train_loader)
    avg_bce = epoch_bce / len(train_loader)
    avg_dice_loss = epoch_dice_loss / len(train_loader)
    
    return avg_loss, avg_dice, avg_bce, avg_dice_loss

# ==================== éªŒè¯å‡½æ•° ====================
def validate():
    """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    epoch_dice = 0
    sample_count = 0
    
    with torch.no_grad():
        for x, mask in val_loader:
            # DataLoaderå·²ç»è‡ªåŠ¨ç»„å¥½batch
            images = x[0].to(device)  # (B, 1, 512, 512)
            mask_batch = mask[1]  # (B, 512, 512)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Dice
            pred_np = outputs.cpu().squeeze(1).numpy()  # (B, 512, 512)
            target_np = mask_batch.cpu().numpy()  # (B, 512, 512)
            
            batch_size = pred_np.shape[0]
            for i in range(batch_size):
                pred_binary = (pred_np[i] >= config.rate).astype(np.float32)
                epoch_dice += dice_loss.dice(pred_binary, target_np[i])
                sample_count += 1
    
    avg_dice = epoch_dice / sample_count if sample_count > 0 else 0
    return avg_dice

# ==================== æµ‹è¯•å‡½æ•° ====================
def test():
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆä»…åœ¨è®­ç»ƒå®Œæˆåä½¿ç”¨ï¼‰"""
    model.eval()
    epoch_dice = 0
    sample_count = 0
    
    with torch.no_grad():
        for x, mask in test_loader:
            images = x[0].to(device)
            mask_batch = mask[1]
            
            outputs = model(images)
            
            pred_np = outputs.cpu().squeeze(1).numpy()
            target_np = mask_batch.cpu().numpy()
            
            batch_size = pred_np.shape[0]
            for i in range(batch_size):
                pred_binary = (pred_np[i] >= config.rate).astype(np.float32)
                epoch_dice += dice_loss.dice(pred_binary, target_np[i])
                sample_count += 1
    
    avg_dice = epoch_dice / sample_count if sample_count > 0 else 0
    return avg_dice

# ==================== ä¸»è®­ç»ƒå¾ªç¯ ====================
print("å¼€å§‹è®­ç»ƒ...\n")
start_time = datetime.now()

for epoch in range(1, config.epochs + 1):
    # è®­ç»ƒ
    train_loss, train_dice, train_bce, train_dice_loss = train_one_epoch(epoch)
    
    # éªŒè¯
    val_dice = validate()
    
    # å­¦ä¹ ç‡è°ƒåº¦
    if config.scheduler_type == 'cosine':
        scheduler.step()
    else:
        scheduler.step(val_dice)
    
    current_lr = optimizer.param_groups[0]['lr']
    
    # è®°å½•å†å²
    history['epoch'].append(epoch)
    history['train_loss'].append(train_loss)
    history['train_dice'].append(train_dice)
    history['val_dice'].append(val_dice)
    history['learning_rate'].append(current_lr)
    history['bce_loss'].append(train_bce)
    history['dice_loss'].append(train_dice_loss)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"Epoch {epoch}/{config.epochs} ç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} (BCE: {train_bce:.4f}, Dice: {train_dice_loss:.4f})")
    print(f"è®­ç»ƒDice: {train_dice:.4f}")
    print(f"éªŒè¯Dice: {val_dice:.4f}")
    print(f"å­¦ä¹ ç‡:   {current_lr:.6f}")
    print(f"{'='*60}\n")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºéªŒè¯é›†)
    if val_dice > best_dice:
        best_dice = val_dice
        best_model_path = f'{config.save_dir}/{config.model_save_name}_best.pth'
        torch.save(model.state_dict(), best_model_path)
        print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯Dice: {best_dice:.4f})\n")
    
    # Early Stoppingæ£€æŸ¥ (åŸºäºéªŒè¯é›†)
    if early_stopping(val_dice):
        print(f"âš ï¸ Early Stoppingè§¦å‘ (Patience: {config.patience})")
        break

# è®­ç»ƒç»“æŸ
end_time = datetime.now()
training_time = (end_time - start_time).total_seconds()

# åœ¨è®­ç»ƒå®Œæˆå,åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
print(f"\n{'='*60}")
print("åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
print(f"{'='*60}")
test_dice = test()

print(f"\n{'='*60}")
print(f"âœ¨ è®­ç»ƒå®Œæˆ!")
print(f"{'='*60}")
print(f"æ€»è€—æ—¶: {training_time/3600:.2f} å°æ—¶")
print(f"æœ€ä½³éªŒè¯Dice: {best_dice:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•Dice: {test_dice:.4f}")
print(f"{'='*60}\n")

# ==================== ä¿å­˜ç»“æœ ====================
# ä¿å­˜æœ€ç»ˆæ¨¡å‹
final_model_path = f'{config.save_dir}/{config.model_save_name}_final.pth'
torch.save(model.state_dict(), final_model_path)

# ä¿å­˜è®­ç»ƒå†å²
history_path = f'{config.log_dir}/{config.model_save_name}_history.json'
with open(history_path, 'w') as f:
    json.dump(history, f, indent=4)

# ==================== ç»˜åˆ¶è®­ç»ƒæ›²çº¿ ====================
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Lossæ›²çº¿
axes[0, 0].plot(history['epoch'], history['train_loss'], label='Total Loss', linewidth=2)
axes[0, 0].plot(history['epoch'], history['bce_loss'], label='BCE Loss', linewidth=2, alpha=0.7)
axes[0, 0].plot(history['epoch'], history['dice_loss'], label='Dice Loss', linewidth=2, alpha=0.7)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('Training Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Diceæ›²çº¿
axes[0, 1].plot(history['epoch'], history['train_dice'], label='Train Dice', linewidth=2)
axes[0, 1].plot(history['epoch'], history['val_dice'], label='Val Dice', linewidth=2)
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Dice Score')
axes[0, 1].set_title('Dice Score Progression')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)
axes[0, 1].axhline(y=0.80, color='r', linestyle='--', alpha=0.5, label='Target (0.80)')

# å­¦ä¹ ç‡æ›²çº¿
axes[1, 0].plot(history['epoch'], history['learning_rate'], linewidth=2, color='green')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Learning Rate')
axes[1, 0].set_title('Learning Rate Schedule')
axes[1, 0].set_yscale('log')
axes[1, 0].grid(True, alpha=0.3)

# æ€§èƒ½å¯¹æ¯”
axes[1, 1].bar(['Train Dice', 'Val Dice', 'Test Dice'], 
               [history['train_dice'][-1], history['val_dice'][-1], test_dice],
               color=['#4CAF50', '#2196F3', '#FF9800'])
axes[1, 1].set_ylabel('Dice Score')
axes[1, 1].set_title('Final Performance')
axes[1, 1].set_ylim([0, 1])
axes[1, 1].axhline(y=0.80, color='r', linestyle='--', alpha=0.5)
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plot_path = f'{config.log_dir}/{config.model_save_name}_curves.png'
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° {plot_path}\n")

# ==================== ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š ====================
report = f"""
# TransUNetè®­ç»ƒæŠ¥å‘Š - {config.model_save_name}

## è®­ç»ƒé…ç½®
- æ¨¡å‹ç±»å‹: {config.model_type}
- å‚æ•°é‡: {total_params / 1e6:.2f}M
- æ‰¹å¤§å°: {config.batch_size}
- åˆå§‹å­¦ä¹ ç‡: {config.learning_rate}
- æ€»Epochs: {epoch}
- è®­ç»ƒæ—¶é•¿: {training_time/3600:.2f} å°æ—¶
- æ•°æ®åˆ’åˆ†: 6:2:2 (éšæœºæ‚£è€…çº§åˆ«)

## æ€§èƒ½æŒ‡æ ‡
- æœ€ä½³éªŒè¯Dice: {best_dice:.4f} ({best_dice*100:.2f}%)
- æœ€ç»ˆæµ‹è¯•Dice: {test_dice:.4f} ({test_dice*100:.2f}%)
- æœ€ç»ˆè®­ç»ƒDice: {history['train_dice'][-1]:.4f}
- è¿‡æ‹Ÿåˆç¨‹åº¦: {abs(history['train_dice'][-1] - test_dice):.4f}

## æŸå¤±å‡½æ•°
- BCEæƒé‡: {config.bce_weight}
- Diceæƒé‡: {config.dice_weight}
- æœ€ç»ˆBCE Loss: {history['bce_loss'][-1]:.4f}
- æœ€ç»ˆDice Loss: {history['dice_loss'][-1]:.4f}

## ä¸Baselineå¯¹æ¯” (UNet+AttentionGate)
- Baseline Dice: éœ€è¦åœ¨æ–°åˆ’åˆ†ä¸Šé‡æ–°è®­ç»ƒåå¯¹æ¯”
- TransUNet Dice: {best_dice:.4f}

## ä¿å­˜çš„æ¨¡å‹
- æœ€ä½³æ¨¡å‹: {config.save_dir}/{config.model_save_name}_best.pth
- æœ€ç»ˆæ¨¡å‹: {config.save_dir}/{config.model_save_name}_final.pth

## è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

report_path = f'{config.log_dir}/{config.model_save_name}_report.md'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write(report)

print(report)
print(f"ğŸ“„ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ° {report_path}\n")
print(f"ğŸ“¦ æœ€ä½³æ¨¡å‹: {config.save_dir}/{config.model_save_name}_best.pth")
print(f"ğŸ“¦ æœ€ç»ˆæ¨¡å‹: {final_model_path}\n")
print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
